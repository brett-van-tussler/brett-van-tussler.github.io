{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Practice Tutorial\n",
    "\n",
    "This notebook provides a comprehensive introduction to pandas, Python's most popular data manipulation and analysis library, along with practical exercises to reinforce your learning.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Pandas](#introduction)\n",
    "2. [Installation](#installation)\n",
    "3. [Core Data Structures](#data-structures)\n",
    "4. [Data Loading and Saving](#data-io)\n",
    "5. [Data Exploration](#exploration)\n",
    "6. [Data Cleaning](#cleaning)\n",
    "7. [Data Manipulation](#manipulation)\n",
    "8. [Grouping and Aggregation](#grouping)\n",
    "9. [Merging and Joining](#merging)\n",
    "10. [Time Series Analysis](#time-series)\n",
    "11. [Practice Exercises](#exercises)\n",
    "12. [Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pandas {#introduction}\n",
    "\n",
    "Pandas is a powerful, open-source data analysis and manipulation library built on top of NumPy. It provides data structures and functions needed to work with structured data seamlessly.\n",
    "\n",
    "### Key Features:\n",
    "- **DataFrame and Series**: Primary data structures for handling structured data\n",
    "- **Data Alignment**: Automatic alignment of data based on labels\n",
    "- **Missing Data Handling**: Robust tools for dealing with missing data\n",
    "- **Data Import/Export**: Read/write data from various formats (CSV, Excel, JSON, SQL, etc.)\n",
    "- **Data Transformation**: Powerful tools for reshaping, pivoting, and transforming data\n",
    "- **Time Series**: Comprehensive time series functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation {#installation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas using pip\n",
    "!pip install pandas\n",
    "\n",
    "# Install additional dependencies for full functionality\n",
    "!pip install openpyxl xlrd matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Data Structures {#data-structures}\n",
    "\n",
    "### Series\n",
    "A Series is a one-dimensional labeled array capable of holding any data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Series\n",
    "data = [10, 20, 30, 40, 50]\n",
    "index = ['a', 'b', 'c', 'd', 'e']\n",
    "series = pd.Series(data, index=index, name='my_series')\n",
    "print(\"Series:\")\n",
    "print(series)\n",
    "print(f\"\\nData type: {series.dtype}\")\n",
    "print(f\"Index: {series.index.tolist()}\")\n",
    "print(f\"Values: {series.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series from dictionary\n",
    "dict_data = {'apple': 5, 'banana': 3, 'orange': 8, 'grape': 12}\n",
    "fruit_series = pd.Series(dict_data)\n",
    "print(\"\\nFruit Series:\")\n",
    "print(fruit_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "A DataFrame is a two-dimensional labeled data structure with columns of potentially different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame from dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['New York', 'London', 'Tokyo', 'Paris', 'Sydney'],\n",
    "    'Salary': [50000, 60000, 70000, 55000, 65000],\n",
    "    'Department': ['IT', 'Finance', 'IT', 'HR', 'Finance']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Index: {df.index.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame info\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Saving {#data-io}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstration\n",
    "sample_data = {\n",
    "    'product_id': range(1, 101),\n",
    "    'product_name': [f'Product_{i}' for i in range(1, 101)],\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 100),\n",
    "    'price': np.random.uniform(10, 500, 100).round(2),\n",
    "    'quantity_sold': np.random.randint(1, 100, 100),\n",
    "    'rating': np.random.uniform(1, 5, 100).round(1),\n",
    "    'date_sold': pd.date_range('2023-01-01', periods=100, freq='D')\n",
    "}\n",
    "\n",
    "products_df = pd.DataFrame(sample_data)\n",
    "print(\"Sample Products DataFrame:\")\n",
    "print(products_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "products_df.to_csv('products.csv', index=False)\n",
    "print(\"Data saved to products.csv\")\n",
    "\n",
    "# Read from CSV\n",
    "loaded_df = pd.read_csv('products.csv')\n",
    "print(\"\\nLoaded DataFrame:\")\n",
    "print(loaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other file formats\n",
    "# Save to Excel\n",
    "products_df.to_excel('products.xlsx', index=False, sheet_name='Products')\n",
    "\n",
    "# Save to JSON\n",
    "products_df.to_json('products.json', orient='records', date_format='iso')\n",
    "\n",
    "# Read from JSON\n",
    "json_df = pd.read_json('products.json')\n",
    "print(\"\\nJSON DataFrame shape:\", json_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration {#exploration}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic exploration methods\n",
    "print(\"First 5 rows:\")\n",
    "print(products_df.head())\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(products_df.tail())\n",
    "\n",
    "print(\"\\nRandom sample:\")\n",
    "print(products_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(products_df.describe())\n",
    "\n",
    "print(\"\\nValue counts for category:\")\n",
    "print(products_df['category'].value_counts())\n",
    "\n",
    "print(\"\\nUnique values in category:\")\n",
    "print(products_df['category'].unique())\n",
    "print(f\"Number of unique categories: {products_df['category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data selection and indexing\n",
    "print(\"Select single column:\")\n",
    "print(products_df['product_name'].head())\n",
    "\n",
    "print(\"\\nSelect multiple columns:\")\n",
    "print(products_df[['product_name', 'price', 'category']].head())\n",
    "\n",
    "print(\"\\nSelect rows by index:\")\n",
    "print(products_df.iloc[0:3])  # First 3 rows\n",
    "\n",
    "print(\"\\nSelect rows by condition:\")\n",
    "expensive_products = products_df[products_df['price'] > 400]\n",
    "print(f\"Products with price > 400: {len(expensive_products)}\")\n",
    "print(expensive_products[['product_name', 'price']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning {#cleaning}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with missing values for demonstration\n",
    "clean_data = products_df.copy()\n",
    "# Introduce some missing values\n",
    "clean_data.loc[5:10, 'rating'] = np.nan\n",
    "clean_data.loc[15:20, 'price'] = np.nan\n",
    "clean_data.loc[25, 'category'] = np.nan\n",
    "\n",
    "print(\"Missing values:\")\n",
    "print(clean_data.isnull().sum())\n",
    "\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((clean_data.isnull().sum() / len(clean_data) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "# Drop rows with any missing values\n",
    "clean_df_dropped = clean_data.dropna()\n",
    "print(f\"Original shape: {clean_data.shape}\")\n",
    "print(f\"After dropping NaN: {clean_df_dropped.shape}\")\n",
    "\n",
    "# Fill missing values\n",
    "clean_df_filled = clean_data.copy()\n",
    "clean_df_filled['rating'].fillna(clean_df_filled['rating'].mean(), inplace=True)\n",
    "clean_df_filled['price'].fillna(clean_df_filled['price'].median(), inplace=True)\n",
    "clean_df_filled['category'].fillna('Unknown', inplace=True)\n",
    "\n",
    "print(\"\\nAfter filling missing values:\")\n",
    "print(clean_df_filled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "print(f\"Original shape: {clean_df_filled.shape}\")\n",
    "clean_df_no_duplicates = clean_df_filled.drop_duplicates()\n",
    "print(f\"After removing duplicates: {clean_df_no_duplicates.shape}\")\n",
    "\n",
    "# Data type conversion\n",
    "print(\"\\nData types before conversion:\")\n",
    "print(clean_df_filled.dtypes)\n",
    "\n",
    "# Convert date column to datetime\n",
    "clean_df_filled['date_sold'] = pd.to_datetime(clean_df_filled['date_sold'])\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(clean_df_filled.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation {#manipulation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns\n",
    "df_manipulated = clean_df_filled.copy()\n",
    "df_manipulated['revenue'] = df_manipulated['price'] * df_manipulated['quantity_sold']\n",
    "df_manipulated['price_category'] = pd.cut(df_manipulated['price'], \n",
    "                                         bins=[0, 50, 150, 300, 500], \n",
    "                                         labels=['Low', 'Medium', 'High', 'Premium'])\n",
    "\n",
    "print(\"DataFrame with new columns:\")\n",
    "print(df_manipulated[['product_name', 'price', 'quantity_sold', 'revenue', 'price_category']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting data\n",
    "print(\"Top 5 products by revenue:\")\n",
    "top_revenue = df_manipulated.nlargest(5, 'revenue')\n",
    "print(top_revenue[['product_name', 'price', 'quantity_sold', 'revenue']])\n",
    "\n",
    "print(\"\\nSorted by price (descending):\")\n",
    "sorted_by_price = df_manipulated.sort_values('price', ascending=False)\n",
    "print(sorted_by_price[['product_name', 'price']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String operations\n",
    "print(\"String operations:\")\n",
    "df_manipulated['product_name_upper'] = df_manipulated['product_name'].str.upper()\n",
    "df_manipulated['product_name_length'] = df_manipulated['product_name'].str.len()\n",
    "df_manipulated['contains_product'] = df_manipulated['product_name'].str.contains('Product')\n",
    "\n",
    "print(df_manipulated[['product_name', 'product_name_upper', 'product_name_length', 'contains_product']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Aggregation {#grouping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by category\n",
    "category_stats = df_manipulated.groupby('category').agg({\n",
    "    'price': ['mean', 'min', 'max'],\n",
    "    'quantity_sold': 'sum',\n",
    "    'revenue': ['sum', 'mean'],\n",
    "    'rating': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Statistics by category:\")\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple grouping\n",
    "price_category_stats = df_manipulated.groupby(['category', 'price_category']).agg({\n",
    "    'revenue': 'sum',\n",
    "    'quantity_sold': 'mean',\n",
    "    'rating': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nStatistics by category and price category:\")\n",
    "print(price_category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot tables\n",
    "pivot_table = df_manipulated.pivot_table(\n",
    "    values='revenue',\n",
    "    index='category',\n",
    "    columns='price_category',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"\\nPivot table - Revenue by Category and Price Category:\")\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Joining {#merging}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional DataFrames for merging\n",
    "suppliers = pd.DataFrame({\n",
    "    'product_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'supplier_name': ['Supplier_A', 'Supplier_B', 'Supplier_A', 'Supplier_C', 'Supplier_B',\n",
    "                     'Supplier_A', 'Supplier_C', 'Supplier_B', 'Supplier_A', 'Supplier_C'],\n",
    "    'supplier_country': ['USA', 'Germany', 'USA', 'Japan', 'Germany',\n",
    "                        'USA', 'Japan', 'Germany', 'USA', 'Japan']\n",
    "})\n",
    "\n",
    "print(\"Suppliers DataFrame:\")\n",
    "print(suppliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "merged_df = pd.merge(df_manipulated, suppliers, on='product_id', how='left')\n",
    "print(\"\\nMerged DataFrame (first 10 rows):\")\n",
    "print(merged_df[['product_name', 'category', 'price', 'supplier_name', 'supplier_country']].head(10))\n",
    "\n",
    "print(f\"\\nOriginal shape: {df_manipulated.shape}\")\n",
    "print(f\"Merged shape: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different types of joins\n",
    "inner_join = pd.merge(df_manipulated, suppliers, on='product_id', how='inner')\n",
    "outer_join = pd.merge(df_manipulated, suppliers, on='product_id', how='outer')\n",
    "\n",
    "print(f\"Inner join shape: {inner_join.shape}\")\n",
    "print(f\"Outer join shape: {outer_join.shape}\")\n",
    "print(f\"Left join shape: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis {#time-series}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series operations\n",
    "df_time = merged_df.copy()\n",
    "df_time['date_sold'] = pd.to_datetime(df_time['date_sold'])\n",
    "df_time.set_index('date_sold', inplace=True)\n",
    "\n",
    "print(\"Time series DataFrame:\")\n",
    "print(df_time.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data by month\n",
    "monthly_sales = df_time.resample('M').agg({\n",
    "    'revenue': 'sum',\n",
    "    'quantity_sold': 'sum',\n",
    "    'price': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nMonthly sales summary:\")\n",
    "print(monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling statistics\n",
    "df_time['revenue_7day_avg'] = df_time['revenue'].rolling(window=7).mean()\n",
    "df_time['revenue_cumsum'] = df_time['revenue'].cumsum()\n",
    "\n",
    "print(\"\\nTime series with rolling statistics:\")\n",
    "print(df_time[['revenue', 'revenue_7day_avg', 'revenue_cumsum']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises {#exercises}\n",
    "\n",
    "### Exercise 1: Sales Analysis\n",
    "**Task**: Analyze the sales data to answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Sales Analysis\n",
    "print(\"=== EXERCISE 1: SALES ANALYSIS ===\")\n",
    "print(\"\\nYour tasks:\")\n",
    "print(\"1. Find the top 3 categories by total revenue\")\n",
    "print(\"2. Calculate the average rating for each price category\")\n",
    "print(\"3. Identify products with rating above 4.5 and price below 100\")\n",
    "print(\"4. Create a summary showing total quantity sold by supplier country\")\n",
    "\n",
    "# Solution space - try to solve before looking at the solution\n",
    "def exercise_1_solution():\n",
    "    # Task 1: Top 3 categories by total revenue\n",
    "    top_categories = merged_df.groupby('category')['revenue'].sum().nlargest(3)\n",
    "    print(\"\\n1. Top 3 categories by total revenue:\")\n",
    "    print(top_categories)\n",
    "    \n",
    "    # Task 2: Average rating by price category\n",
    "    avg_rating_by_price = merged_df.groupby('price_category')['rating'].mean().round(2)\n",
    "    print(\"\\n2. Average rating by price category:\")\n",
    "    print(avg_rating_by_price)\n",
    "    \n",
    "    # Task 3: High-rated, low-priced products\n",
    "    high_rated_low_price = merged_df[(merged_df['rating'] > 4.5) & (merged_df['price'] < 100)]\n",
    "    print(f\"\\n3. Products with rating > 4.5 and price < 100: {len(high_rated_low_price)} products\")\n",
    "    if len(high_rated_low_price) > 0:\n",
    "        print(high_rated_low_price[['product_name', 'price', 'rating']].head())\n",
    "    \n",
    "    # Task 4: Quantity sold by supplier country\n",
    "    qty_by_country = merged_df.groupby('supplier_country')['quantity_sold'].sum().sort_values(ascending=False)\n",
    "    print(\"\\n4. Total quantity sold by supplier country:\")\n",
    "    print(qty_by_country)\n",
    "\n",
    "# Uncomment to see the solution\n",
    "# exercise_1_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Data Transformation\n",
    "**Task**: Transform and clean the data according to specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Data Transformation\n",
    "print(\"\\n=== EXERCISE 2: DATA TRANSFORMATION ===\")\n",
    "print(\"\\nYour tasks:\")\n",
    "print(\"1. Create a new column 'profit_margin' assuming 30% profit margin\")\n",
    "print(\"2. Categorize products as 'Bestseller' (quantity_sold > 75) or 'Regular'\")\n",
    "print(\"3. Create a pivot table showing average price by category and price_category\")\n",
    "print(\"4. Find the month with the highest total revenue\")\n",
    "\n",
    "def exercise_2_solution():\n",
    "    df_ex2 = merged_df.copy()\n",
    "    \n",
    "    # Task 1: Profit margin\n",
    "    df_ex2['profit_margin'] = df_ex2['price'] * 0.30\n",
    "    print(\"\\n1. Added profit_margin column\")\n",
    "    print(df_ex2[['product_name', 'price', 'profit_margin']].head())\n",
    "    \n",
    "    # Task 2: Bestseller categorization\n",
    "    df_ex2['sales_category'] = df_ex2['quantity_sold'].apply(lambda x: 'Bestseller' if x > 75 else 'Regular')\n",
    "    print(\"\\n2. Sales category distribution:\")\n",
    "    print(df_ex2['sales_category'].value_counts())\n",
    "    \n",
    "    # Task 3: Pivot table\n",
    "    price_pivot = df_ex2.pivot_table(\n",
    "        values='price',\n",
    "        index='category',\n",
    "        columns='price_category',\n",
    "        aggfunc='mean'\n",
    "    ).round(2)\n",
    "    print(\"\\n3. Average price by category and price category:\")\n",
    "    print(price_pivot)\n",
    "    \n",
    "    # Task 4: Month with highest revenue\n",
    "    df_ex2['month'] = pd.to_datetime(df_ex2['date_sold']).dt.to_period('M')\n",
    "    monthly_revenue = df_ex2.groupby('month')['revenue'].sum()\n",
    "    highest_month = monthly_revenue.idxmax()\n",
    "    highest_revenue = monthly_revenue.max()\n",
    "    print(f\"\\n4. Month with highest revenue: {highest_month} (${highest_revenue:,.2f})\")\n",
    "\n",
    "# Uncomment to see the solution\n",
    "# exercise_2_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Advanced Analysis\n",
    "**Task**: Perform advanced data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Advanced Analysis\n",
    "print(\"\\n=== EXERCISE 3: ADVANCED ANALYSIS ===\")\n",
    "print(\"\\nYour tasks:\")\n",
    "print(\"1. Calculate correlation between price and rating\")\n",
    "print(\"2. Find the supplier with the highest average product rating\")\n",
    "print(\"3. Create a time series showing daily revenue trend\")\n",
    "print(\"4. Identify outliers in the price column using IQR method\")\n",
    "\n",
    "def exercise_3_solution():\n",
    "    # Task 1: Correlation between price and rating\n",
    "    correlation = merged_df['price'].corr(merged_df['rating'])\n",
    "    print(f\"\\n1. Correlation between price and rating: {correlation:.3f}\")\n",
    "    \n",
    "    # Task 2: Supplier with highest average rating\n",
    "    supplier_ratings = merged_df.groupby('supplier_name')['rating'].mean().sort_values(ascending=False)\n",
    "    print(\"\\n2. Suppliers by average product rating:\")\n",
    "    print(supplier_ratings)\n",
    "    \n",
    "    # Task 3: Daily revenue trend\n",
    "    daily_revenue = merged_df.groupby('date_sold')['revenue'].sum()\n",
    "    print(\"\\n3. Daily revenue trend (first 10 days):\")\n",
    "    print(daily_revenue.head(10))\n",
    "    \n",
    "    # Task 4: Price outliers using IQR\n",
    "    Q1 = merged_df['price'].quantile(0.25)\n",
    "    Q3 = merged_df['price'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = merged_df[(merged_df['price'] < lower_bound) | (merged_df['price'] > upper_bound)]\n",
    "    print(f\"\\n4. Price outliers (IQR method): {len(outliers)} products\")\n",
    "    print(f\"   Lower bound: ${lower_bound:.2f}, Upper bound: ${upper_bound:.2f}\")\n",
    "    if len(outliers) > 0:\n",
    "        print(\"   Outlier products:\")\n",
    "        print(outliers[['product_name', 'price']].head())\n",
    "\n",
    "# Uncomment to see the solution\n",
    "# exercise_3_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with Pandas {#visualization}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plotting with pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Revenue by category\n",
    "category_revenue = merged_df.groupby('category')['revenue'].sum()\n",
    "category_revenue.plot(kind='bar', ax=axes[0,0], title='Total Revenue by Category')\n",
    "axes[0,0].set_ylabel('Revenue ($)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Price distribution\n",
    "merged_df['price'].hist(bins=20, ax=axes[0,1], title='Price Distribution')\n",
    "axes[0,1].set_xlabel('Price ($)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Rating vs Price scatter plot\n",
    "merged_df.plot.scatter(x='price', y='rating', ax=axes[1,0], title='Rating vs Price')\n",
    "axes[1,0].set_xlabel('Price ($)')\n",
    "axes[1,0].set_ylabel('Rating')\n",
    "\n",
    "# 4. Daily revenue trend\n",
    "daily_revenue = merged_df.groupby('date_sold')['revenue'].sum()\n",
    "daily_revenue.plot(ax=axes[1,1], title='Daily Revenue Trend')\n",
    "axes[1,1].set_xlabel('Date')\n",
    "axes[1,1].set_ylabel('Revenue ($)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization plots created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tips {#performance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance tips for pandas\n",
    "print(\"=== PANDAS PERFORMANCE TIPS ===\")\n",
    "\n",
    "# 1. Use vectorized operations instead of loops\n",
    "import time\n",
    "\n",
    "# Slow way (using loops)\n",
    "start_time = time.time()\n",
    "result_slow = []\n",
    "for price in merged_df['price']:\n",
    "    result_slow.append(price * 1.1)\n",
    "slow_time = time.time() - start_time\n",
    "\n",
    "# Fast way (vectorized)\n",
    "start_time = time.time()\n",
    "result_fast = merged_df['price'] * 1.1\n",
    "fast_time = time.time() - start_time\n",
    "\n",
    "print(f\"Loop method time: {slow_time:.6f} seconds\")\n",
    "print(f\"Vectorized method time: {fast_time:.6f} seconds\")\n",
    "print(f\"Speedup: {slow_time/fast_time:.1f}x faster\")\n",
    "\n",
    "# 2. Use appropriate data types\n",
    "print(\"\\n2. Memory usage optimization:\")\n",
    "print(f\"Original memory usage: {merged_df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Optimize data types\n",
    "df_optimized = merged_df.copy()\n",
    "df_optimized['product_id'] = df_optimized['product_id'].astype('int16')\n",
    "df_optimized['quantity_sold'] = df_optimized['quantity_sold'].astype('int8')\n",
    "df_optimized['category'] = df_optimized['category'].astype('category')\n",
    "df_optimized['supplier_name'] = df_optimized['supplier_name'].astype('category')\n",
    "\n",
    "print(f\"Optimized memory usage: {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(f\"Memory reduction: {(1 - df_optimized.memory_usage(deep=True).sum() / merged_df.memory_usage(deep=True).sum()) * 100:.1f}%\")\n",
    "\n",
    "# 3. Use query() for complex filtering\n",
    "print(\"\\n3. Query method for filtering:\")\n",
    "# Traditional filtering\n",
    "filtered_traditional = merged_df[(merged_df['price'] > 100) & (merged_df['rating'] > 4.0) & (merged_df['category'] == 'Electronics')]\n",
    "\n",
    "# Using query (more readable)\n",
    "filtered_query = merged_df.query('price > 100 and rating > 4.0 and category == \"Electronics\"')\n",
    "\n",
    "print(f\"Traditional filtering result: {len(filtered_traditional)} rows\")\n",
    "print(f\"Query method result: {len(filtered_query)} rows\")\n",
    "print(\"Query method is more readable and often faster for complex conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls and Best Practices {#best-practices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== COMMON PITFALLS AND BEST PRACTICES ===\")\n",
    "\n",
    "# 1. Chained assignment warning\n",
    "print(\"1. Avoiding chained assignment:\")\n",
    "# Bad practice (may cause SettingWithCopyWarning)\n",
    "# df_subset = merged_df[merged_df['price'] > 100]\n",
    "# df_subset['new_column'] = 'value'  # This might not work as expected\n",
    "\n",
    "# Good practice\n",
    "df_subset = merged_df[merged_df['price'] > 100].copy()\n",
    "df_subset['new_column'] = 'value'\n",
    "print(\"Use .copy() when creating subsets that you plan to modify\")\n",
    "\n",
    "# 2. Efficient string operations\n",
    "print(\"\\n2. Efficient string operations:\")\n",
    "# Use vectorized string methods\n",
    "product_names_upper = merged_df['product_name'].str.upper()\n",
    "print(\"Use .str accessor for vectorized string operations\")\n",
    "\n",
    "# 3. Memory-efficient iteration\n",
    "print(\"\\n3. Memory-efficient iteration:\")\n",
    "# Use itertuples() instead of iterrows() for better performance\n",
    "print(\"Use itertuples() instead of iterrows() for iteration\")\n",
    "for row in merged_df.head(3).itertuples():\n",
    "    print(f\"Product {row.product_id}: {row.product_name} - ${row.price}\")\n",
    "\n",
    "# 4. Proper handling of missing data\n",
    "print(\"\\n4. Missing data best practices:\")\n",
    "print(\"- Always check for missing data before analysis\")\n",
    "print(\"- Choose appropriate strategy: drop, fill, or interpolate\")\n",
    "print(\"- Document your missing data handling decisions\")\n",
    "\n",
    "# 5. Index usage\n",
    "print(\"\\n5. Efficient index usage:\")\n",
    "print(\"- Set meaningful indexes for faster lookups\")\n",
    "print(\"- Use .loc and .iloc for explicit indexing\")\n",
    "print(\"- Reset index when necessary to avoid confusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources {#resources}\n",
    "\n",
    "### Official Documentation\n",
    "- [Pandas Official Documentation](https://pandas.pydata.org/docs/)\n",
    "- [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
    "- [Pandas Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html)\n",
    "\n",
    "### Recommended Books\n",
    "- \"Python for Data Analysis\" by Wes McKinney (creator of pandas)\n",
    "- \"Pandas in Action\" by Boris Paskhaver\n",
    "- \"Effective Pandas\" by Matt Harrison\n",
    "\n",
    "### Online Resources\n",
    "- [Kaggle Learn - Pandas Course](https://www.kaggle.com/learn/pandas)\n",
    "- [DataCamp Pandas Tutorials](https://www.datacamp.com/tutorial/pandas)\n",
    "- [Real Python Pandas Tutorials](https://realpython.com/pandas-python-explore-dataset/)\n",
    "\n",
    "### Practice Datasets\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "- [FiveThirtyEight Data](https://github.com/fivethirtyeight/data)\n",
    "- [Our World in Data](https://ourworldindata.org/)\n",
    "\n",
    "### Advanced Topics to Explore\n",
    "1. **Multi-indexing**: Working with hierarchical indexes\n",
    "2. **Categorical Data**: Efficient handling of categorical variables\n",
    "3. **Time Series**: Advanced time series analysis and forecasting\n",
    "4. **Performance Optimization**: Using Dask for larger-than-memory datasets\n",
    "5. **Integration**: Working with SQL databases, APIs, and other data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered the essential aspects of pandas:\n",
    "\n",
    "✅ **Core Concepts**: Series and DataFrame structures\n",
    "✅ **Data I/O**: Reading and writing various file formats\n",
    "✅ **Data Exploration**: Basic statistics and data inspection\n",
    "✅ **Data Cleaning**: Handling missing values and duplicates\n",
    "✅ **Data Manipulation**: Filtering, sorting, and transforming data\n",
    "✅ **Grouping & Aggregation**: Summarizing data by groups\n",
    "✅ **Merging & Joining**: Combining multiple datasets\n",
    "✅ **Time Series**: Working with temporal data\n",
    "✅ **Performance**: Optimization techniques and best practices\n",
    "\n",
    "### Next Steps\n",
    "1. Practice with real datasets from Kaggle or other sources\n",
    "2. Explore advanced pandas features like multi-indexing\n",
    "3. Learn complementary libraries (NumPy, Matplotlib, Seaborn)\n",
    "4. Consider Dask for big data scenarios\n",
    "5. Integrate pandas with machine learning workflows\n",
    "\n",
    "Remember: The key to mastering pandas is practice! Start with small datasets and gradually work your way up to more complex analyses.\n",
    "\n",
    "---\n",
    "\n",
    "*Happy data analyzing! 🐼📊*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}