{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Practice Tutorial\n",
    "\n",
    "This notebook provides a comprehensive introduction to pandas, Python's most popular data manipulation and analysis library, along with practical exercises to reinforce your learning.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Pandas](#Introduction-to-Pandas)\n",
    "2. [Installation](#Installation)\n",
    "3. [Core Data Structures](#Core-Data-Structures)\n",
    "4. [Data Loading and Saving](#Data-Loading-and-Saving)\n",
    "5. [Data Exploration](#Data-Exploration)\n",
    "6. [Data Cleaning](#Data-Cleaning)\n",
    "7. [Data Manipulation](#Data-Manipulation)\n",
    "8. [Grouping and Aggregation](#Grouping-and-Aggregation)\n",
    "9. [Merging and Joining](#Merging-and-Joining)\n",
    "10. [Time Series Analysis](#Time-Series-Analysis)\n",
    "11. [Practice Exercises](#Practice-Exercises)\n",
    "12. [Visualization with Pandas](#Visualization-with-Pandas)\n",
    "13. [Performance Tips](#Performance-Tips)\n",
    "14. [Common Pitfalls and Best Practices](#Common-Pitfalls-and-Best-Practices)\n",
    "15. [Additional Resources](#Additional-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pandas\n",
    "\n",
    "Pandas is a powerful, open-source data analysis and manipulation library built on top of NumPy. It provides data structures and functions needed to work with structured data seamlessly.\n",
    "\n",
    "### Key Features:\n",
    "- **DataFrame and Series**: Primary data structures for handling structured data\n",
    "- **Data Alignment**: Automatic alignment of data based on labels\n",
    "- **Missing Data Handling**: Robust tools for dealing with missing data\n",
    "- **Data Import/Export**: Read/write data from various formats (CSV, Excel, JSON, SQL, etc.)\n",
    "- **Data Transformation**: Powerful tools for reshaping, pivoting, and transforming data\n",
    "- **Time Series**: Comprehensive time series functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas using pip\n",
    "!pip install pandas\n",
    "\n",
    "# Install additional dependencies for full functionality\n",
    "!pip install openpyxl xlrd matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.3.0\n",
      "NumPy version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Data Structures\n",
    "\n",
    "### Series\n",
    "A Series is a one-dimensional labeled array capable of holding any data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series:\n",
      "a    10\n",
      "b    20\n",
      "c    30\n",
      "d    40\n",
      "e    50\n",
      "Name: my_series, dtype: int64\n",
      "\n",
      "Data type: int64\n",
      "Index: ['a', 'b', 'c', 'd', 'e']\n",
      "Values: [10 20 30 40 50]\n"
     ]
    }
   ],
   "source": [
    "# Creating a Series\n",
    "data = [10, 20, 30, 40, 50]\n",
    "index = ['a', 'b', 'c', 'd', 'e']\n",
    "series = pd.Series(data, index=index, name='my_series')\n",
    "print(\"Series:\")\n",
    "print(series)\n",
    "print(f\"\\nData type: {series.dtype}\")\n",
    "print(f\"Index: {series.index.tolist()}\")\n",
    "print(f\"Values: {series.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fruit Series:\n",
      "apple      5\n",
      "banana     3\n",
      "orange     8\n",
      "grape     12\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Series from dictionary\n",
    "dict_data = {'apple': 5, 'banana': 3, 'orange': 8, 'grape': 12}\n",
    "fruit_series = pd.Series(dict_data)\n",
    "print(\"\\nFruit Series:\")\n",
    "print(fruit_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "A DataFrame is a two-dimensional labeled data structure with columns of potentially different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "      Name  Age      City  Salary Department\n",
      "0    Alice   25  New York   50000         IT\n",
      "1      Bob   30    London   60000    Finance\n",
      "2  Charlie   35     Tokyo   70000         IT\n",
      "3    Diana   28     Paris   55000         HR\n",
      "4      Eve   32    Sydney   65000    Finance\n",
      "\n",
      "Shape: (5, 5)\n",
      "Columns: ['Name', 'Age', 'City', 'Salary', 'Department']\n",
      "Index: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame from dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['New York', 'London', 'Tokyo', 'Paris', 'Sydney'],\n",
    "    'Salary': [50000, 60000, 70000, 55000, 65000],\n",
    "    'Department': ['IT', 'Finance', 'IT', 'HR', 'Finance']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Index: {df.index.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Name        5 non-null      object\n",
      " 1   Age         5 non-null      int64 \n",
      " 2   City        5 non-null      object\n",
      " 3   Salary      5 non-null      int64 \n",
      " 4   Department  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 332.0+ bytes\n",
      "None\n",
      "\n",
      "Data Types:\n",
      "Name          object\n",
      "Age            int64\n",
      "City          object\n",
      "Salary         int64\n",
      "Department    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# DataFrame info\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Products DataFrame:\n",
      "   product_id product_name     category   price  quantity_sold  rating  \\\n",
      "0           1    Product_1     Clothing  164.32             63     4.8   \n",
      "1           2    Product_2         Home  421.23              2     4.4   \n",
      "2           3    Product_3  Electronics  163.21             28     2.3   \n",
      "3           4    Product_4         Home  230.79              5     3.2   \n",
      "4           5    Product_5  Electronics  250.13             13     3.5   \n",
      "\n",
      "   date_sold  \n",
      "0 2023-01-01  \n",
      "1 2023-01-02  \n",
      "2 2023-01-03  \n",
      "3 2023-01-04  \n",
      "4 2023-01-05  \n"
     ]
    }
   ],
   "source": [
    "# Create sample data for demonstration\n",
    "sample_data = {\n",
    "    'product_id': range(1, 101),\n",
    "    'product_name': [f'Product_{i}' for i in range(1, 101)],\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 100),\n",
    "    'price': np.random.uniform(10, 500, 100).round(2),\n",
    "    'quantity_sold': np.random.randint(1, 100, 100),\n",
    "    'rating': np.random.uniform(1, 5, 100).round(1),\n",
    "    'date_sold': pd.date_range('2023-01-01', periods=100, freq='D')\n",
    "}\n",
    "\n",
    "products_df = pd.DataFrame(sample_data)\n",
    "print(\"Sample Products DataFrame:\")\n",
    "print(products_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "products_df.to_csv('products.csv', index=False)\n",
    "print(\"Data saved to products.csv\")\n",
    "\n",
    "# Read from CSV\n",
    "loaded_df = pd.read_csv('products.csv')\n",
    "print(\"\\nLoaded DataFrame:\")\n",
    "print(loaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other file formats\n",
    "# Save to Excel\n",
    "products_df.to_excel('products.xlsx', index=False, sheet_name='Products')\n",
    "\n",
    "# Save to JSON\n",
    "products_df.to_json('products.json', orient='records', date_format='iso')\n",
    "\n",
    "# Read from JSON\n",
    "json_df = pd.read_json('products.json')\n",
    "print(\"\\nJSON DataFrame shape:\", json_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "   product_id product_name     category   price  quantity_sold  rating  \\\n",
      "0           1    Product_1     Clothing  164.32             63     4.8   \n",
      "1           2    Product_2         Home  421.23              2     4.4   \n",
      "2           3    Product_3  Electronics  163.21             28     2.3   \n",
      "3           4    Product_4         Home  230.79              5     3.2   \n",
      "4           5    Product_5  Electronics  250.13             13     3.5   \n",
      "\n",
      "   date_sold  \n",
      "0 2023-01-01  \n",
      "1 2023-01-02  \n",
      "2 2023-01-03  \n",
      "3 2023-01-04  \n",
      "4 2023-01-05  \n",
      "\n",
      "Last 5 rows:\n",
      "    product_id product_name     category   price  quantity_sold  rating  \\\n",
      "95          96   Product_96     Clothing  265.26             59     4.0   \n",
      "96          97   Product_97  Electronics  111.48             84     1.6   \n",
      "97          98   Product_98     Clothing   31.88             87     1.3   \n",
      "98          99   Product_99  Electronics   43.30             36     1.4   \n",
      "99         100  Product_100        Books  336.88             27     1.0   \n",
      "\n",
      "    date_sold  \n",
      "95 2023-04-06  \n",
      "96 2023-04-07  \n",
      "97 2023-04-08  \n",
      "98 2023-04-09  \n",
      "99 2023-04-10  \n",
      "\n",
      "Random sample:\n",
      "    product_id product_name     category   price  quantity_sold  rating  \\\n",
      "2            3    Product_3  Electronics  163.21             28     2.3   \n",
      "31          32   Product_32         Home  281.94             55     4.4   \n",
      "1            2    Product_2         Home  421.23              2     4.4   \n",
      "\n",
      "    date_sold  \n",
      "2  2023-01-03  \n",
      "31 2023-02-01  \n",
      "1  2023-01-02  \n"
     ]
    }
   ],
   "source": [
    "# Basic exploration methods\n",
    "print(\"First 5 rows:\")\n",
    "print(products_df.head())\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(products_df.tail())\n",
    "\n",
    "print(\"\\nRandom sample:\")\n",
    "print(products_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Summary:\n",
      "       product_id       price  quantity_sold      rating            date_sold\n",
      "count  100.000000  100.000000     100.000000  100.000000                  100\n",
      "mean    50.500000  250.066500      49.030000    2.923000  2023-02-19 12:00:00\n",
      "min      1.000000   12.160000       2.000000    1.000000  2023-01-01 00:00:00\n",
      "25%     25.750000  141.990000      27.000000    1.900000  2023-01-25 18:00:00\n",
      "50%     50.500000  250.025000      47.500000    2.800000  2023-02-19 12:00:00\n",
      "75%     75.250000  346.825000      75.000000    4.025000  2023-03-16 06:00:00\n",
      "max    100.000000  499.020000      99.000000    5.000000  2023-04-10 00:00:00\n",
      "std     29.011492  131.681611      28.236896    1.172139                  NaN\n",
      "\n",
      "Value counts for category:\n",
      "category\n",
      "Electronics    28\n",
      "Home           26\n",
      "Books          24\n",
      "Clothing       22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values in category:\n",
      "['Clothing' 'Home' 'Electronics' 'Books']\n",
      "Number of unique categories: 4\n"
     ]
    }
   ],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(products_df.describe())\n",
    "\n",
    "print(\"\\nValue counts for category:\")\n",
    "print(products_df['category'].value_counts())\n",
    "\n",
    "print(\"\\nUnique values in category:\")\n",
    "print(products_df['category'].unique())\n",
    "print(f\"Number of unique categories: {products_df['category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select single column:\n",
      "0    Product_1\n",
      "1    Product_2\n",
      "2    Product_3\n",
      "3    Product_4\n",
      "4    Product_5\n",
      "Name: product_name, dtype: object\n",
      "\n",
      "Select multiple columns:\n",
      "  product_name   price     category\n",
      "0    Product_1  164.32     Clothing\n",
      "1    Product_2  421.23         Home\n",
      "2    Product_3  163.21  Electronics\n",
      "3    Product_4  230.79         Home\n",
      "4    Product_5  250.13  Electronics\n",
      "\n",
      "Select rows by index:\n",
      "   product_id product_name     category   price  quantity_sold  rating  \\\n",
      "0           1    Product_1     Clothing  164.32             63     4.8   \n",
      "1           2    Product_2         Home  421.23              2     4.4   \n",
      "2           3    Product_3  Electronics  163.21             28     2.3   \n",
      "\n",
      "   date_sold  \n",
      "0 2023-01-01  \n",
      "1 2023-01-02  \n",
      "2 2023-01-03  \n",
      "\n",
      "Select rows by condition:\n",
      "Products with price > 400: 16\n",
      "   product_name   price\n",
      "1     Product_2  421.23\n",
      "14   Product_15  441.03\n",
      "17   Product_18  437.33\n",
      "21   Product_22  499.02\n",
      "29   Product_30  484.49\n"
     ]
    }
   ],
   "source": [
    "# Data selection and indexing\n",
    "print(\"Select single column:\")\n",
    "print(products_df['product_name'].head())\n",
    "\n",
    "print(\"\\nSelect multiple columns:\")\n",
    "print(products_df[['product_name', 'price', 'category']].head())\n",
    "\n",
    "print(\"\\nSelect rows by index:\")\n",
    "print(products_df.iloc[0:3])  # First 3 rows\n",
    "\n",
    "print(\"\\nSelect rows by condition:\")\n",
    "expensive_products = products_df[products_df['price'] > 400]\n",
    "print(f\"Products with price > 400: {len(expensive_products)}\")\n",
    "print(expensive_products[['product_name', 'price']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "product_id       0\n",
      "product_name     0\n",
      "category         1\n",
      "price            6\n",
      "quantity_sold    0\n",
      "rating           6\n",
      "date_sold        0\n",
      "dtype: int64\n",
      "\n",
      "Percentage of missing values:\n",
      "product_id       0.0\n",
      "product_name     0.0\n",
      "category         1.0\n",
      "price            6.0\n",
      "quantity_sold    0.0\n",
      "rating           6.0\n",
      "date_sold        0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create data with missing values for demonstration\n",
    "clean_data = products_df.copy()\n",
    "# Introduce some missing values\n",
    "clean_data.loc[5:10, 'rating'] = np.nan\n",
    "clean_data.loc[15:20, 'price'] = np.nan\n",
    "clean_data.loc[25, 'category'] = np.nan\n",
    "\n",
    "print(\"Missing values:\")\n",
    "print(clean_data.isnull().sum())\n",
    "\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((clean_data.isnull().sum() / len(clean_data) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (100, 7)\n",
      "After dropping NaN: (87, 7)\n",
      "\n",
      "After filling missing values:\n",
      "product_id       0\n",
      "product_name     0\n",
      "category         0\n",
      "price            0\n",
      "quantity_sold    0\n",
      "rating           0\n",
      "date_sold        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handling missing values\n",
    "# Drop rows with any missing values\n",
    "clean_df_dropped = clean_data.dropna()\n",
    "print(f\"Original shape: {clean_data.shape}\")\n",
    "print(f\"After dropping NaN: {clean_df_dropped.shape}\")\n",
    "\n",
    "# Fill missing values\n",
    "clean_df_filled = clean_data.copy()\n",
    "clean_df_filled['rating'] = clean_df_filled['rating'].fillna(clean_df_filled['rating'].mean())\n",
    "clean_df_filled['price'] = clean_df_filled['price'].fillna(clean_df_filled['price'].median())\n",
    "clean_df_filled['category'] = clean_df_filled['category'].fillna('Unknown')\n",
    "\n",
    "print(\"\\nAfter filling missing values:\")\n",
    "print(clean_df_filled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (100, 7)\n",
      "After removing duplicates: (100, 7)\n",
      "\n",
      "Data types before conversion:\n",
      "product_id                int64\n",
      "product_name             object\n",
      "category                 object\n",
      "price                   float64\n",
      "quantity_sold             int64\n",
      "rating                  float64\n",
      "date_sold        datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Data types after conversion:\n",
      "product_id                int64\n",
      "product_name             object\n",
      "category                 object\n",
      "price                   float64\n",
      "quantity_sold             int64\n",
      "rating                  float64\n",
      "date_sold        datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "print(f\"Original shape: {clean_df_filled.shape}\")\n",
    "clean_df_no_duplicates = clean_df_filled.drop_duplicates()\n",
    "print(f\"After removing duplicates: {clean_df_no_duplicates.shape}\")\n",
    "\n",
    "# Data type conversion\n",
    "print(\"\\nData types before conversion:\")\n",
    "print(clean_df_filled.dtypes)\n",
    "\n",
    "# Convert date column to datetime\n",
    "clean_df_filled['date_sold'] = pd.to_datetime(clean_df_filled['date_sold'])\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(clean_df_filled.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Practice Problems: Data Cleaning\n",
    "\n",
    "**Problem 1**: Create a DataFrame with missing values and practice different strategies to handle them. Create a dataset with columns ['name', 'age', 'salary'] where some values are missing, then fill missing ages with the median and missing salaries with the mean.\n",
    "\n",
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution\n",
    "import numpy as np\n",
    "\n",
    "# Create sample data with missing values\n",
    "data_with_missing = {\n",
    "    'name': ['John', 'Jane', 'Bob', 'Alice', 'Charlie'],\n",
    "    'age': [25, np.nan, 35, 28, np.nan],\n",
    "    'salary': [50000, 60000, np.nan, 55000, np.nan]\n",
    "}\n",
    "\n",
    "df_missing = pd.DataFrame(data_with_missing)\n",
    "print('Original DataFrame:')\n",
    "print(df_missing)\n",
    "print('\\nMissing values:')\n",
    "print(df_missing.isnull().sum())\n",
    "\n",
    "# Fill missing values\n",
    "df_cleaned = df_missing.copy()\n",
    "df_cleaned['age'] = df_cleaned['age'].fillna(df_cleaned['age'].median())\n",
    "df_cleaned['salary'] = df_cleaned['salary'].fillna(df_cleaned['salary'].mean())\n",
    "\n",
    "print('\\nCleaned DataFrame:')\n",
    "print(df_cleaned)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "**Problem 2**: Remove duplicate rows from a DataFrame and convert a string column to proper datetime format.\n",
    "\n",
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution\n",
    "# Create sample data with duplicates\n",
    "messy_data = {\n",
    "    'product': ['A', 'B', 'A', 'C', 'B'],\n",
    "    'price': [100, 200, 100, 300, 200],\n",
    "    'date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-02']\n",
    "}\n",
    "\n",
    "df_messy = pd.DataFrame(messy_data)\n",
    "print('Original DataFrame:')\n",
    "print(df_messy)\n",
    "print(f'Shape: {df_messy.shape}')\n",
    "\n",
    "# Remove duplicates\n",
    "df_clean = df_messy.drop_duplicates()\n",
    "print(f'\\nAfter removing duplicates: {df_clean.shape}')\n",
    "\n",
    "# Convert date column\n",
    "df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
    "print('\\nData types after conversion:')\n",
    "print(df_clean.dtypes)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Problem 1 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Problem 2 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with new columns:\n",
      "  product_name   price  quantity_sold   revenue price_category\n",
      "0    Product_1  164.32             63  10352.16           High\n",
      "1    Product_2  421.23              2    842.46        Premium\n",
      "2    Product_3  163.21             28   4569.88           High\n",
      "3    Product_4  230.79              5   1153.95           High\n",
      "4    Product_5  250.13             13   3251.69           High\n"
     ]
    }
   ],
   "source": [
    "# Adding new columns\n",
    "df_manipulated = clean_df_filled.copy()\n",
    "df_manipulated['revenue'] = df_manipulated['price'] * df_manipulated['quantity_sold']\n",
    "df_manipulated['price_category'] = pd.cut(df_manipulated['price'], \n",
    "                                         bins=[0, 50, 150, 300, 500], \n",
    "                                         labels=['Low', 'Medium', 'High', 'Premium'])\n",
    "\n",
    "print(\"DataFrame with new columns:\")\n",
    "print(df_manipulated[['product_name', 'price', 'quantity_sold', 'revenue', 'price_category']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 products by revenue:\n",
      "   product_name   price  quantity_sold   revenue\n",
      "38   Product_39  476.55             93  44319.15\n",
      "21   Product_22  499.02             75  37426.50\n",
      "91   Product_92  431.52             69  29774.88\n",
      "80   Product_81  479.57             59  28294.63\n",
      "58   Product_59  445.77             62  27637.74\n",
      "\n",
      "Sorted by price (descending):\n",
      "   product_name   price\n",
      "21   Product_22  499.02\n",
      "86   Product_87  491.82\n",
      "29   Product_30  484.49\n",
      "48   Product_49  481.95\n",
      "80   Product_81  479.57\n"
     ]
    }
   ],
   "source": [
    "# Sorting data\n",
    "print(\"Top 5 products by revenue:\")\n",
    "top_revenue = df_manipulated.nlargest(5, 'revenue')\n",
    "print(top_revenue[['product_name', 'price', 'quantity_sold', 'revenue']])\n",
    "\n",
    "print(\"\\nSorted by price (descending):\")\n",
    "sorted_by_price = df_manipulated.sort_values('price', ascending=False)\n",
    "print(sorted_by_price[['product_name', 'price']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String operations:\n",
      "  product_name product_name_upper  product_name_length  contains_product\n",
      "0    Product_1          PRODUCT_1                    9              True\n",
      "1    Product_2          PRODUCT_2                    9              True\n",
      "2    Product_3          PRODUCT_3                    9              True\n",
      "3    Product_4          PRODUCT_4                    9              True\n",
      "4    Product_5          PRODUCT_5                    9              True\n"
     ]
    }
   ],
   "source": [
    "# String operations\n",
    "print(\"String operations:\")\n",
    "df_manipulated['product_name_upper'] = df_manipulated['product_name'].str.upper()\n",
    "df_manipulated['product_name_length'] = df_manipulated['product_name'].str.len()\n",
    "df_manipulated['contains_product'] = df_manipulated['product_name'].str.contains('Product')\n",
    "\n",
    "print(df_manipulated[['product_name', 'product_name_upper', 'product_name_length', 'contains_product']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Practice Problems: Data Manipulation\n",
    "\n",
    "**Problem 1**: Create a new column 'price_per_unit' by dividing price by quantity_sold. Then create a categorical column 'performance' that assigns 'High' for ratings ≥4.0, 'Medium' for ratings 2.5-3.9, and 'Low' for ratings <2.5.\n",
    "\n",
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution\n",
    "df_practice = products_df.copy()\n",
    "\n",
    "# Create price per unit column\n",
    "df_practice['price_per_unit'] = df_practice['price'] / df_practice['quantity_sold']\n",
    "\n",
    "# Create performance categories\n",
    "def categorize_performance(rating):\n",
    "    if rating >= 4.0:\n",
    "        return 'High'\n",
    "    elif rating >= 2.5:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "df_practice['performance'] = df_practice['rating'].apply(categorize_performance)\n",
    "\n",
    "print('New columns added:')\n",
    "print(df_practice[['product_name', 'price', 'quantity_sold', 'price_per_unit', 'rating', 'performance']].head())\n",
    "print(f'\\nPerformance distribution:')\n",
    "print(df_practice['performance'].value_counts())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "**Problem 2**: Filter the DataFrame to show only products where the product_name contains a number greater than 50, and sort them by price in descending order.\n",
    "\n",
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution\n",
    "# Extract number from product name and filter\n",
    "df_practice['product_number'] = df_practice['product_name'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Filter products with number > 50\n",
    "high_number_products = df_practice[df_practice['product_number'] > 50]\n",
    "\n",
    "# Sort by price descending\n",
    "result = high_number_products.sort_values('price', ascending=False)\n",
    "\n",
    "print(f'Products with number > 50: {len(result)} products')\n",
    "print(result[['product_name', 'product_number', 'price', 'category']].head(10))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Problem 1 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Problem 2 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity_sold</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_sold</th>\n",
       "      <th>revenue</th>\n",
       "      <th>price_category</th>\n",
       "      <th>product_name_upper</th>\n",
       "      <th>product_name_length</th>\n",
       "      <th>contains_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Product_1</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>164.32</td>\n",
       "      <td>63</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10352.16</td>\n",
       "      <td>High</td>\n",
       "      <td>PRODUCT_1</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Product_2</td>\n",
       "      <td>Home</td>\n",
       "      <td>421.23</td>\n",
       "      <td>2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>842.46</td>\n",
       "      <td>Premium</td>\n",
       "      <td>PRODUCT_2</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Product_3</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>163.21</td>\n",
       "      <td>28</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>4569.88</td>\n",
       "      <td>High</td>\n",
       "      <td>PRODUCT_3</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Product_4</td>\n",
       "      <td>Home</td>\n",
       "      <td>230.79</td>\n",
       "      <td>5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1153.95</td>\n",
       "      <td>High</td>\n",
       "      <td>PRODUCT_4</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Product_5</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>250.13</td>\n",
       "      <td>13</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>3251.69</td>\n",
       "      <td>High</td>\n",
       "      <td>PRODUCT_5</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Product_96</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>265.26</td>\n",
       "      <td>59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>15650.34</td>\n",
       "      <td>High</td>\n",
       "      <td>PRODUCT_96</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Product_97</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>111.48</td>\n",
       "      <td>84</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>9364.32</td>\n",
       "      <td>Medium</td>\n",
       "      <td>PRODUCT_97</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Product_98</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>31.88</td>\n",
       "      <td>87</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2023-04-08</td>\n",
       "      <td>2773.56</td>\n",
       "      <td>Low</td>\n",
       "      <td>PRODUCT_98</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Product_99</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>43.30</td>\n",
       "      <td>36</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2023-04-09</td>\n",
       "      <td>1558.80</td>\n",
       "      <td>Low</td>\n",
       "      <td>PRODUCT_99</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Product_100</td>\n",
       "      <td>Books</td>\n",
       "      <td>336.88</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-04-10</td>\n",
       "      <td>9095.76</td>\n",
       "      <td>Premium</td>\n",
       "      <td>PRODUCT_100</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_id product_name     category   price  quantity_sold  rating  \\\n",
       "0            1    Product_1     Clothing  164.32             63     4.8   \n",
       "1            2    Product_2         Home  421.23              2     4.4   \n",
       "2            3    Product_3  Electronics  163.21             28     2.3   \n",
       "3            4    Product_4         Home  230.79              5     3.2   \n",
       "4            5    Product_5  Electronics  250.13             13     3.5   \n",
       "..         ...          ...          ...     ...            ...     ...   \n",
       "95          96   Product_96     Clothing  265.26             59     4.0   \n",
       "96          97   Product_97  Electronics  111.48             84     1.6   \n",
       "97          98   Product_98     Clothing   31.88             87     1.3   \n",
       "98          99   Product_99  Electronics   43.30             36     1.4   \n",
       "99         100  Product_100        Books  336.88             27     1.0   \n",
       "\n",
       "    date_sold   revenue price_category product_name_upper  \\\n",
       "0  2023-01-01  10352.16           High          PRODUCT_1   \n",
       "1  2023-01-02    842.46        Premium          PRODUCT_2   \n",
       "2  2023-01-03   4569.88           High          PRODUCT_3   \n",
       "3  2023-01-04   1153.95           High          PRODUCT_4   \n",
       "4  2023-01-05   3251.69           High          PRODUCT_5   \n",
       "..        ...       ...            ...                ...   \n",
       "95 2023-04-06  15650.34           High         PRODUCT_96   \n",
       "96 2023-04-07   9364.32         Medium         PRODUCT_97   \n",
       "97 2023-04-08   2773.56            Low         PRODUCT_98   \n",
       "98 2023-04-09   1558.80            Low         PRODUCT_99   \n",
       "99 2023-04-10   9095.76        Premium        PRODUCT_100   \n",
       "\n",
       "    product_name_length  contains_product  \n",
       "0                     9              True  \n",
       "1                     9              True  \n",
       "2                     9              True  \n",
       "3                     9              True  \n",
       "4                     9              True  \n",
       "..                  ...               ...  \n",
       "95                   10              True  \n",
       "96                   10              True  \n",
       "97                   10              True  \n",
       "98                   10              True  \n",
       "99                   11              True  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manipulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics by category:\n",
      "              price                 quantity_sold    revenue           rating\n",
      "               mean     min     max           sum        sum      mean   mean\n",
      "category                                                                     \n",
      "Books        291.54   85.45  481.95          1068  297858.00  12950.35   2.76\n",
      "Clothing     266.72   31.88  484.49          1104  265183.80  12053.81   2.73\n",
      "Electronics  213.34   42.76  431.52          1355  278155.99   9934.14   2.98\n",
      "Home         266.66   12.16  499.02          1339  327758.07  12606.08   3.18\n",
      "Unknown      120.76  120.76  120.76            37    4468.12   4468.12   3.50\n"
     ]
    }
   ],
   "source": [
    "# Group by category\n",
    "category_stats = df_manipulated.groupby('category').agg({\n",
    "    'price': ['mean', 'min', 'max'],\n",
    "    'quantity_sold': 'sum',\n",
    "    'revenue': ['sum', 'mean'],\n",
    "    'rating': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Statistics by category:\")\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics by category and price category:\n",
      "                              revenue  quantity_sold  rating\n",
      "category    price_category                                  \n",
      "Books       Low                  0.00            NaN     NaN\n",
      "            Medium            1025.40          12.00    2.60\n",
      "            High            158701.51          57.08    2.78\n",
      "            Premium         138131.09          37.10    2.76\n",
      "Clothing    Low               2773.56          87.00    1.30\n",
      "            Medium           20899.02          66.00    3.21\n",
      "            High            134658.83          51.55    3.09\n",
      "            Premium         106852.39          36.00    2.17\n",
      "Electronics Low               4979.60          58.00    3.15\n",
      "            Medium           53786.33          51.44    2.69\n",
      "            High            137749.29          51.73    2.82\n",
      "            Premium          81640.77          34.50    3.63\n",
      "Home        Low               4775.18          84.00    4.55\n",
      "            Medium           16289.94          68.00    3.63\n",
      "            High            116777.28          48.27    3.38\n",
      "            Premium         189915.67          43.60    2.56\n",
      "Unknown     Low                  0.00            NaN     NaN\n",
      "            Medium            4468.12          37.00    3.50\n",
      "            High                 0.00            NaN     NaN\n",
      "            Premium              0.00            NaN     NaN\n"
     ]
    }
   ],
   "source": [
    "# Multiple grouping\n",
    "price_category_stats = df_manipulated.groupby(['category', 'price_category'], observed=False).agg({\n",
    "    'revenue': 'sum',\n",
    "    'quantity_sold': 'mean',\n",
    "    'rating': ['mean', 'sum\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nStatistics by category and price category:\")\n",
    "print(price_category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot tables\n",
    "pivot_table = df_manipulated.pivot_table(\n",
    "    values='revenue',\n",
    "    index='category',\n",
    "    columns='price_category',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"\\nPivot table - Revenue by Category and Price Category:\")\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 1: Sales Analysis by Category\n",
    "\n",
    "Using the `df_manipulated` DataFrame, create a grouped analysis that shows:\n",
    "1. Total revenue by category\n",
    "2. Average rating by category\n",
    "3. Count of products by category\n",
    "4. Maximum price by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Sales Analysis by Category\n",
    "category_analysis = df_manipulated.groupby('category').agg({\n",
    "    'revenue': 'sum',\n",
    "    'rating': 'mean',\n",
    "    'product_id': 'count',\n",
    "    'price': 'max'\n",
    "}).round(2)\n",
    "\n",
    "category_analysis.columns = ['Total Revenue', 'Avg Rating', 'Product Count', 'Max Price']\n",
    "print(category_analysis)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 2: Advanced Pivot Analysis\n",
    "\n",
    "Create a pivot table that shows:\n",
    "1. Average quantity sold for each combination of category and price_category\n",
    "2. Fill missing values with 0\n",
    "3. Add row and column totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Advanced Pivot Analysis\n",
    "pivot_qty = df_manipulated.pivot_table(\n",
    "    values='quantity_sold',\n",
    "    index='category',\n",
    "    columns='price_category',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0,\n",
    "    margins=True\n",
    ").round(2)\n",
    "\n",
    "print('Average Quantity Sold by Category and Price Category:')\n",
    "print(pivot_qty)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional DataFrames for merging\n",
    "suppliers = pd.DataFrame({\n",
    "    'product_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'supplier_name': ['Supplier_A', 'Supplier_B', 'Supplier_A', 'Supplier_C', 'Supplier_B',\n",
    "                     'Supplier_A', 'Supplier_C', 'Supplier_B', 'Supplier_A', 'Supplier_C'],\n",
    "    'supplier_country': ['USA', 'Germany', 'USA', 'Japan', 'Germany',\n",
    "                        'USA', 'Japan', 'Germany', 'USA', 'Japan']\n",
    "})\n",
    "\n",
    "print(\"Suppliers DataFrame:\")\n",
    "print(suppliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "merged_df = pd.merge(df_manipulated, suppliers, on='product_id', how='left')\n",
    "print(\"\\nMerged DataFrame (first 10 rows):\")\n",
    "print(merged_df[['product_name', 'category', 'price', 'supplier_name', 'supplier_country']].head(10))\n",
    "\n",
    "print(f\"\\nOriginal shape: {df_manipulated.shape}\")\n",
    "print(f\"Merged shape: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different types of joins\n",
    "inner_join = pd.merge(df_manipulated, suppliers, on='product_id', how='inner')\n",
    "outer_join = pd.merge(df_manipulated, suppliers, on='product_id', how='outer')\n",
    "\n",
    "print(f\"Inner join shape: {inner_join.shape}\")\n",
    "print(f\"Outer join shape: {outer_join.shape}\")\n",
    "print(f\"Left join shape: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 1: Data Merging Challenge\n",
    "\n",
    "Create a new DataFrame called `inventory` with the following data:\n",
    "- product_id: [1, 2, 3, 11, 12, 13]\n",
    "- stock_level: [50, 30, 75, 20, 40, 60]\n",
    "- warehouse: ['A', 'B', 'A', 'C', 'B', 'A']\n",
    "\n",
    "Then perform different types of joins with the main DataFrame and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Data Merging Challenge\n",
    "inventory = pd.DataFrame({\n",
    "    'product_id': [1, 2, 3, 11, 12, 13],\n",
    "    'stock_level': [50, 30, 75, 20, 40, 60],\n",
    "    'warehouse': ['A', 'B', 'A', 'C', 'B', 'A']\n",
    "})\n",
    "\n",
    "# Inner join - only matching records\n",
    "inner_result = pd.merge(df_manipulated, inventory, on='product_id', how='inner')\n",
    "print(f'Inner join shape: {inner_result.shape}')\n",
    "\n",
    "# Left join - all records from main DataFrame\n",
    "left_result = pd.merge(df_manipulated, inventory, on='product_id', how='left')\n",
    "print(f'Left join shape: {left_result.shape}')\n",
    "\n",
    "# Outer join - all records from both DataFrames\n",
    "outer_result = pd.merge(df_manipulated, inventory, on='product_id', how='outer')\n",
    "print(f'Outer join shape: {outer_result.shape}')\n",
    "\n",
    "# Check for missing values after left join\n",
    "print(f'Missing stock_level values: {left_result[\"stock_level\"].isna().sum()}')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 2: Supplier Analysis\n",
    "\n",
    "Using the merged DataFrame with suppliers:\n",
    "1. Find the supplier with the highest total revenue\n",
    "2. Calculate average price by supplier country\n",
    "3. Identify which supplier country has the most diverse product categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Supplier Analysis\n",
    "# 1. Supplier with highest total revenue\n",
    "supplier_revenue = merged_df.groupby('supplier_name')['revenue'].sum().sort_values(ascending=False)\n",
    "print('Supplier revenue ranking:')\n",
    "print(supplier_revenue)\n",
    "print(f'Top supplier: {supplier_revenue.index[0]}')\n",
    "\n",
    "# 2. Average price by supplier country\n",
    "country_avg_price = merged_df.groupby('supplier_country')['price'].mean().round(2)\n",
    "print('\\nAverage price by supplier country:')\n",
    "print(country_avg_price)\n",
    "\n",
    "# 3. Most diverse product categories by country\n",
    "category_diversity = merged_df.groupby('supplier_country')['category'].nunique().sort_values(ascending=False)\n",
    "print('\\nCategory diversity by supplier country:')\n",
    "print(category_diversity)\n",
    "print(f'Most diverse country: {category_diversity.index[0]}')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series operations\n",
    "df_time = merged_df.copy()\n",
    "df_time['date_sold'] = pd.to_datetime(df_time['date_sold'])\n",
    "df_time.set_index('date_sold', inplace=True)\n",
    "\n",
    "print(\"Time series DataFrame:\")\n",
    "print(df_time.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data by month\n",
    "monthly_sales = df_time.resample('M').agg({\n",
    "    'revenue': 'sum',\n",
    "    'quantity_sold': 'sum',\n",
    "    'price': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nMonthly sales summary:\")\n",
    "print(monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling statistics\n",
    "df_time['revenue_7day_avg'] = df_time['revenue'].rolling(window=7).mean()\n",
    "df_time['revenue_cumsum'] = df_time['revenue'].cumsum()\n",
    "\n",
    "print(\"\\nTime series with rolling statistics:\")\n",
    "print(df_time[['revenue', 'revenue_7day_avg', 'revenue_cumsum']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 1: Time Series Resampling\n",
    "\n",
    "Using the time series DataFrame:\n",
    "1. Resample the data by week and calculate total revenue and average rating\n",
    "2. Find the week with the highest total revenue\n",
    "3. Calculate the percentage change in weekly revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Time Series Resampling\n",
    "# 1. Weekly resampling\n",
    "weekly_stats = df_time.resample('W').agg({\n",
    "    'revenue': 'sum',\n",
    "    'rating': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print('Weekly statistics:')\n",
    "print(weekly_stats)\n",
    "\n",
    "# 2. Week with highest revenue\n",
    "max_revenue_week = weekly_stats['revenue'].idxmax()\n",
    "max_revenue_value = weekly_stats['revenue'].max()\n",
    "print(f'\\nWeek with highest revenue: {max_revenue_week.strftime(\"%Y-%m-%d\")} (${max_revenue_value:,.2f})')\n",
    "\n",
    "# 3. Percentage change in weekly revenue\n",
    "weekly_stats['revenue_pct_change'] = weekly_stats['revenue'].pct_change() * 100\n",
    "print('\\nWeekly revenue percentage change:')\n",
    "print(weekly_stats[['revenue', 'revenue_pct_change']].dropna())\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 2: Rolling Window Analysis\n",
    "\n",
    "Create rolling window calculations:\n",
    "1. Calculate a 5-day rolling average for quantity sold\n",
    "2. Find the 3-day rolling maximum price\n",
    "3. Calculate the rolling standard deviation of revenue (7-day window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Rolling Window Analysis\n",
    "df_rolling = df_time.copy()\n",
    "\n",
    "# 1. 5-day rolling average for quantity sold\n",
    "df_rolling['qty_5day_avg'] = df_rolling['quantity_sold'].rolling(window=5).mean()\n",
    "\n",
    "# 2. 3-day rolling maximum price\n",
    "df_rolling['price_3day_max'] = df_rolling['price'].rolling(window=3).max()\n",
    "\n",
    "# 3. 7-day rolling standard deviation of revenue\n",
    "df_rolling['revenue_7day_std'] = df_rolling['revenue'].rolling(window=7).std()\n",
    "\n",
    "print('Rolling window statistics:')\n",
    "print(df_rolling[['quantity_sold', 'qty_5day_avg', 'price', 'price_3day_max', \n",
    "                  'revenue', 'revenue_7day_std']].head(10))\n",
    "\n",
    "# Show summary statistics\n",
    "print('\\nSummary of rolling calculations:')\n",
    "print(df_rolling[['qty_5day_avg', 'price_3day_max', 'revenue_7day_std']].describe())\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Sales Analysis\n",
    "**Task**: Analyze the sales data to answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Sales Analysis\n",
    "print(\"=== EXERCISE 1: SALES ANALYSIS ===\")\n",
    "print(\"\\nYour tasks:\")\n",
    "print(\"1. Find the top 3 categories by total revenue\")\n",
    "print(\"2. Calculate the average rating for each price category\")\n",
    "print(\"3. Identify products with rating above 4.5 and price below 100\")\n",
    "print(\"4. Create a summary showing total quantity sold by supplier country\")\n",
    "\n",
    "# Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Sales Analysis\n",
    "# Task 1: Top 3 categories by total revenue\n",
    "top_categories = merged_df.groupby('category')['revenue'].sum().nlargest(3)\n",
    "print('1. Top 3 categories by total revenue:')\n",
    "print(top_categories)\n",
    "\n",
    "# Task 2: Average rating by price category\n",
    "avg_rating_by_price = merged_df.groupby('price_category')['rating'].mean().round(2)\n",
    "print('\\n2. Average rating by price category:')\n",
    "print(avg_rating_by_price)\n",
    "\n",
    "# Task 3: High-rated, low-priced products\n",
    "high_rated_low_price = merged_df[(merged_df['rating'] > 4.5) & (merged_df['price'] < 100)]\n",
    "print(f'\\n3. Products with rating > 4.5 and price < 100: {len(high_rated_low_price)} products')\n",
    "if len(high_rated_low_price) > 0:\n",
    "    print(high_rated_low_price[['product_name', 'price', 'rating']].head())\n",
    "\n",
    "# Task 4: Quantity sold by supplier country\n",
    "qty_by_country = merged_df.groupby('supplier_country')['quantity_sold'].sum().sort_values(ascending=False)\n",
    "print('\\n4. Total quantity sold by supplier country:')\n",
    "print(qty_by_country)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Data Transformation\n",
    "**Task**: Transform and clean the data according to specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Data Transformation\n",
    "print(\"\\n=== EXERCISE 2: DATA TRANSFORMATION ===\")\n",
    "print(\"\\nYour tasks:\")\n",
    "print(\"1. Create a new column 'profit_margin' assuming 30% profit margin\")\n",
    "print(\"2. Categorize products as 'Bestseller' (quantity_sold > 75) or 'Regular'\")\n",
    "print(\"3. Create a pivot table showing average price by category and price_category\")\n",
    "print(\"4. Find the month with the highest total revenue\")\n",
    "\n",
    "# Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Data Transformation\n",
    "df_ex2 = merged_df.copy()\n",
    "\n",
    "# Task 1: Profit margin\n",
    "df_ex2['profit_margin'] = df_ex2['price'] * 0.30\n",
    "print('1. Added profit_margin column')\n",
    "print(df_ex2[['product_name', 'price', 'profit_margin']].head())\n",
    "\n",
    "# Task 2: Bestseller categorization\n",
    "df_ex2['sales_category'] = df_ex2['quantity_sold'].apply(lambda x: 'Bestseller' if x > 75 else 'Regular')\n",
    "print('\\n2. Sales category distribution:')\n",
    "print(df_ex2['sales_category'].value_counts())\n",
    "\n",
    "# Task 3: Pivot table\n",
    "price_pivot = df_ex2.pivot_table(\n",
    "    values='price',\n",
    "    index='category',\n",
    "    columns='price_category',\n",
    "    aggfunc='mean'\n",
    ").round(2)\n",
    "print('\\n3. Average price by category and price category:')\n",
    "print(price_pivot)\n",
    "\n",
    "# Task 4: Month with highest revenue\n",
    "df_ex2['month'] = pd.to_datetime(df_ex2['date_sold']).dt.to_period('M')\n",
    "monthly_revenue = df_ex2.groupby('month')['revenue'].sum()\n",
    "highest_month = monthly_revenue.idxmax()\n",
    "highest_revenue = monthly_revenue.max()\n",
    "print(f'\\n4. Month with highest revenue: {highest_month} (${highest_revenue:,.2f})')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Advanced Analysis\n",
    "**Task**: Perform advanced data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Advanced Analysis\n",
    "print(\"\\n=== EXERCISE 3: ADVANCED ANALYSIS ===\")\n",
    "print(\"\\nYour tasks:\")\n",
    "print(\"1. Calculate correlation between price and rating\")\n",
    "print(\"2. Find the supplier with the highest average product rating\")\n",
    "print(\"3. Create a time series showing daily revenue trend\")\n",
    "print(\"4. Identify outliers in the price column using IQR method\")\n",
    "\n",
    "# Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "# Solution: Advanced Analysis\n",
    "# Task 1: Correlation between price and rating\n",
    "correlation = merged_df['price'].corr(merged_df['rating'])\n",
    "print(f'1. Correlation between price and rating: {correlation:.3f}')\n",
    "\n",
    "# Task 2: Supplier with highest average rating\n",
    "supplier_ratings = merged_df.groupby('supplier_name')['rating'].mean().sort_values(ascending=False)\n",
    "print('\\n2. Suppliers by average product rating:')\n",
    "print(supplier_ratings)\n",
    "\n",
    "# Task 3: Daily revenue trend\n",
    "daily_revenue = merged_df.groupby('date_sold')['revenue'].sum()\n",
    "print('\\n3. Daily revenue trend (first 10 days):')\n",
    "print(daily_revenue.head(10))\n",
    "\n",
    "# Task 4: Price outliers using IQR\n",
    "Q1 = merged_df['price'].quantile(0.25)\n",
    "Q3 = merged_df['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = merged_df[(merged_df['price'] < lower_bound) | (merged_df['price'] > upper_bound)]\n",
    "print(f'\\n4. Price outliers (IQR method): {len(outliers)} products')\n",
    "print(f'   Lower bound: ${lower_bound:.2f}, Upper bound: ${upper_bound:.2f}')\n",
    "if len(outliers) > 0:\n",
    "    print('   Outlier products:')\n",
    "    print(outliers[['product_name', 'price']].head())\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plotting with pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Revenue by category\n",
    "category_revenue = merged_df.groupby('category')['revenue'].sum()\n",
    "category_revenue.plot(kind='bar', ax=axes[0,0], title='Total Revenue by Category')\n",
    "axes[0,0].set_ylabel('Revenue ($)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Price distribution\n",
    "merged_df['price'].hist(bins=20, ax=axes[0,1], title='Price Distribution')\n",
    "axes[0,1].set_xlabel('Price ($)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Rating vs Price scatter plot\n",
    "merged_df.plot.scatter(x='price', y='rating', ax=axes[1,0], title='Rating vs Price')\n",
    "axes[1,0].set_xlabel('Price ($)')\n",
    "axes[1,0].set_ylabel('Rating')\n",
    "\n",
    "# 4. Daily revenue trend\n",
    "daily_revenue = merged_df.groupby('date_sold')['revenue'].sum()\n",
    "daily_revenue.plot(ax=axes[1,1], title='Daily Revenue Trend')\n",
    "axes[1,1].set_xlabel('Date')\n",
    "axes[1,1].set_ylabel('Revenue ($)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization plots created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance tips for pandas\n",
    "print(\"=== PANDAS PERFORMANCE TIPS ===\")\n",
    "\n",
    "# 1. Use vectorized operations instead of loops\n",
    "import time\n",
    "\n",
    "# Slow way (using loops)\n",
    "start_time = time.time()\n",
    "result_slow = []\n",
    "for price in merged_df['price']:\n",
    "    result_slow.append(price * 1.1)\n",
    "slow_time = time.time() - start_time\n",
    "\n",
    "# Fast way (vectorized)\n",
    "start_time = time.time()\n",
    "result_fast = merged_df['price'] * 1.1\n",
    "fast_time = time.time() - start_time\n",
    "\n",
    "print(f\"Loop method time: {slow_time:.6f} seconds\")\n",
    "print(f\"Vectorized method time: {fast_time:.6f} seconds\")\n",
    "print(f\"Speedup: {slow_time/fast_time:.1f}x faster\")\n",
    "\n",
    "# 2. Use appropriate data types\n",
    "print(\"\\n2. Memory usage optimization:\")\n",
    "print(f\"Original memory usage: {merged_df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Optimize data types\n",
    "df_optimized = merged_df.copy()\n",
    "df_optimized['product_id'] = df_optimized['product_id'].astype('int16')\n",
    "df_optimized['quantity_sold'] = df_optimized['quantity_sold'].astype('int8')\n",
    "df_optimized['category'] = df_optimized['category'].astype('category')\n",
    "df_optimized['supplier_name'] = df_optimized['supplier_name'].astype('category')\n",
    "\n",
    "print(f\"Optimized memory usage: {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(f\"Memory reduction: {(1 - df_optimized.memory_usage(deep=True).sum() / merged_df.memory_usage(deep=True).sum()) * 100:.1f}%\")\n",
    "\n",
    "# 3. Use query() for complex filtering\n",
    "print(\"\\n3. Query method for filtering:\")\n",
    "# Traditional filtering\n",
    "filtered_traditional = merged_df[(merged_df['price'] > 100) & (merged_df['rating'] > 4.0) & (merged_df['category'] == 'Electronics')]\n",
    "\n",
    "# Using query (more readable)\n",
    "filtered_query = merged_df.query('price > 100 and rating > 4.0 and category == \"Electronics\"')\n",
    "\n",
    "print(f\"Traditional filtering result: {len(filtered_traditional)} rows\")\n",
    "print(f\"Query method result: {len(filtered_query)} rows\")\n",
    "print(\"Query method is more readable and often faster for complex conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== COMMON PITFALLS AND BEST PRACTICES ===\")\n",
    "\n",
    "# 1. Chained assignment warning\n",
    "print(\"1. Avoiding chained assignment:\")\n",
    "# Bad practice (may cause SettingWithCopyWarning)\n",
    "# df_subset = merged_df[merged_df['price'] > 100]\n",
    "# df_subset['new_column'] = 'value'  # This might not work as expected\n",
    "\n",
    "# Good practice\n",
    "df_subset = merged_df[merged_df['price'] > 100].copy()\n",
    "df_subset['new_column'] = 'value'\n",
    "print(\"Use .copy() when creating subsets that you plan to modify\")\n",
    "\n",
    "# 2. Efficient string operations\n",
    "print(\"\\n2. Efficient string operations:\")\n",
    "# Use vectorized string methods\n",
    "product_names_upper = merged_df['product_name'].str.upper()\n",
    "print(\"Use .str accessor for vectorized string operations\")\n",
    "\n",
    "# 3. Memory-efficient iteration\n",
    "print(\"\\n3. Memory-efficient iteration:\")\n",
    "# Use itertuples() instead of iterrows() for better performance\n",
    "print(\"Use itertuples() instead of iterrows() for iteration\")\n",
    "for row in merged_df.head(3).itertuples():\n",
    "    print(f\"Product {row.product_id}: {row.product_name} - ${row.price}\")\n",
    "\n",
    "# 4. Proper handling of missing data\n",
    "print(\"\\n4. Missing data best practices:\")\n",
    "print(\"- Always check for missing data before analysis\")\n",
    "print(\"- Choose appropriate strategy: drop, fill, or interpolate\")\n",
    "print(\"- Document your missing data handling decisions\")\n",
    "\n",
    "# 5. Index usage\n",
    "print(\"\\n5. Efficient index usage:\")\n",
    "print(\"- Set meaningful indexes for faster lookups\")\n",
    "print(\"- Use .loc and .iloc for explicit indexing\")\n",
    "print(\"- Reset index when necessary to avoid confusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Official Documentation\n",
    "- [Pandas Official Documentation](https://pandas.pydata.org/docs/)\n",
    "- [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
    "- [Pandas Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html)\n",
    "\n",
    "### Recommended Books\n",
    "- \"Python for Data Analysis\" by Wes McKinney (creator of pandas)\n",
    "- \"Pandas in Action\" by Boris Paskhaver\n",
    "- \"Effective Pandas\" by Matt Harrison\n",
    "\n",
    "### Online Resources\n",
    "- [Kaggle Learn - Pandas Course](https://www.kaggle.com/learn/pandas)\n",
    "- [DataCamp Pandas Tutorials](https://www.datacamp.com/tutorial/pandas)\n",
    "- [Real Python Pandas Tutorials](https://realpython.com/pandas-python-explore-dataset/)\n",
    "\n",
    "### Practice Datasets\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "- [FiveThirtyEight Data](https://github.com/fivethirtyeight/data)\n",
    "- [Our World in Data](https://ourworldindata.org/)\n",
    "\n",
    "### Advanced Topics to Explore\n",
    "1. **Multi-indexing**: Working with hierarchical indexes\n",
    "2. **Categorical Data**: Efficient handling of categorical variables\n",
    "3. **Time Series**: Advanced time series analysis and forecasting\n",
    "4. **Performance Optimization**: Using Dask for larger-than-memory datasets\n",
    "5. **Integration**: Working with SQL databases, APIs, and other data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered the essential aspects of pandas:\n",
    "\n",
    "✅ **Core Concepts**: Series and DataFrame structures\n",
    "✅ **Data I/O**: Reading and writing various file formats\n",
    "✅ **Data Exploration**: Basic statistics and data inspection\n",
    "✅ **Data Cleaning**: Handling missing values and duplicates\n",
    "✅ **Data Manipulation**: Filtering, sorting, and transforming data\n",
    "✅ **Grouping & Aggregation**: Summarizing data by groups\n",
    "✅ **Merging & Joining**: Combining multiple datasets\n",
    "✅ **Time Series**: Working with temporal data\n",
    "✅ **Performance**: Optimization techniques and best practices\n",
    "\n",
    "### Next Steps\n",
    "1. Practice with real datasets from Kaggle or other sources\n",
    "2. Explore advanced pandas features like multi-indexing\n",
    "3. Learn complementary libraries (NumPy, Matplotlib, Seaborn)\n",
    "4. Consider Dask for big data scenarios\n",
    "5. Integrate pandas with machine learning workflows\n",
    "\n",
    "Remember: The key to mastering pandas is practice! Start with small datasets and gradually work your way up to more complex analyses.\n",
    "\n",
    "---\n",
    "\n",
    "*Happy data analyzing! 🐼📊*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
